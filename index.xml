<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nishant Mishra</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Nishant Mishra</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020</copyright><lastBuildDate>Fri, 21 Aug 2020 09:28:39 -0400</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Nishant Mishra</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Transe</title>
      <link>/project/transe/</link>
      <pubDate>Fri, 21 Aug 2020 09:28:39 -0400</pubDate>
      <guid>/project/transe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CharIR</title>
      <link>/project/charir/</link>
      <pubDate>Fri, 21 Aug 2020 09:28:31 -0400</pubDate>
      <guid>/project/charir/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Policy_gradient</title>
      <link>/project/policy_gradient/</link>
      <pubDate>Fri, 21 Aug 2020 09:25:48 -0400</pubDate>
      <guid>/project/policy_gradient/</guid>
      <description></description>
    </item>
    
    <item>
      <title>March Madness</title>
      <link>/project/march-madness/</link>
      <pubDate>Fri, 21 Aug 2020 09:24:13 -0400</pubDate>
      <guid>/project/march-madness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Online_learning</title>
      <link>/project/online_learning/</link>
      <pubDate>Fri, 21 Aug 2020 09:23:37 -0400</pubDate>
      <guid>/project/online_learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speaker_recognition</title>
      <link>/project/speaker_recognition/</link>
      <pubDate>Fri, 21 Aug 2020 09:22:49 -0400</pubDate>
      <guid>/project/speaker_recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cropnet</title>
      <link>/project/cropnet/</link>
      <pubDate>Fri, 21 Aug 2020 09:20:49 -0400</pubDate>
      <guid>/project/cropnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Activity_recognition</title>
      <link>/project/activity_recognition/</link>
      <pubDate>Fri, 21 Aug 2020 09:20:15 -0400</pubDate>
      <guid>/project/activity_recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>OCR_ASR</title>
      <link>/project/ocr_asr/</link>
      <pubDate>Fri, 21 Aug 2020 09:20:00 -0400</pubDate>
      <guid>/project/ocr_asr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Highlighter(Auto field detection)</title>
      <link>/project/highlighter/</link>
      <pubDate>Fri, 21 Aug 2020 09:18:07 -0400</pubDate>
      <guid>/project/highlighter/</guid>
      <description>&lt;p&gt;This project involved an automatic highlighter tool for automatic highlighting and extraction of specific form fields from documents for further processing such as Optical Character Recognition, information retrieval from handwritten documents or even to facilitate semi manual digital population of records from forms using a user interface.&lt;/p&gt;
&lt;p&gt;The tool utilizes document layout detection, classical Computer vision techniques like template matching and mathematical heuristics to create a generalizable automatic highlighting tool using only one sample of the concerned document.&lt;/p&gt;
&lt;p&gt;The associated repository here is designed for handling a particular bank form and is a command line highlighting tool that can be appropriated/extended for other documents and interfaces.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dory OCR</title>
      <link>/project/dory-ocr/</link>
      <pubDate>Tue, 18 Aug 2020 00:34:01 -0400</pubDate>
      <guid>/project/dory-ocr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CV</title>
      <link>/cv/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Portfolio</title>
      <link>/portfolio/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/portfolio/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generic Extraction Module (G.E.M)</title>
      <link>/project/gem/</link>
      <pubDate>Tue, 21 May 2019 09:20:41 -0400</pubDate>
      <guid>/project/gem/</guid>
      <description>&lt;p&gt;The project at Signzy involved training a generalizable model for information retrieval from OCR output of Indian ID cards. We used both character level embeddings and word level embeddings(
&lt;a href=&#34;https://allennlp.org/elmo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ELMO&lt;/a&gt;) in a stacked manner for language modelling before passing the concatenated embeddings to a bidirectional Long Short Term Memory neural network with Conditional Random Field modelling on LSTM output (
&lt;a href=&#34;https://arxiv.org/abs/1508.01991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huang et al.&lt;/a&gt;) for final classification.&lt;/p&gt;
&lt;p&gt;The model was trained on a large corpus of text OCR outputs obtained from our own proprietary ID cards dataset for extracting non-trivial information such as Names, dates, numbers, addreses from any card. The training was done in a way to ensure the embeddings were also fine tuned. The 
&lt;a href=&#34;https://github.com/flairNLP/flair&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FlairNLP library&lt;/a&gt; was used to create the preprocessing, text embedding, training and postprocessing pipeline and training was performed using pytorch framework. Multiple combinations of embeddings including FlairEmbeddings(
&lt;a href=&#34;https://www.aclweb.org/anthology/C18-1139/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contextualized string embeddings for sequence labelling&lt;/a&gt;), BERT, CharacterEmbeddings, ELMO, XLNet were benchmarked before settling on the final pair based on accuracy, compute and efficiency considerations.&lt;/p&gt;
&lt;p&gt;Not only did the model perform admirably well on unseen text from ID types part of training data irrespective of variations in OCR output and image layout, but it generalised well for out of sample ID types too when finetuned with just 1-5 samples of these cards.&lt;/p&gt;
&lt;p&gt;The idea behind this was to build a generic, flexible information retrieval engine thats pretrained to extract important information from OCR output of all ID cards without specifically being trained on them or having seen them, without any rule based processing, that can be easily finetuned on a very small number of samples of any new card type for optimum performance. This was made into a rest API as a plug and play product for clients to finetune the model on their samples and then use it out of the box to extract information from IDs. The performance was measured using precision and recall figures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IQA</title>
      <link>/project/iqa/</link>
      <pubDate>Mon, 21 Jan 2019 09:20:23 -0400</pubDate>
      <guid>/project/iqa/</guid>
      <description>&lt;p&gt;Many of the vision based applications or APIs meant for information retrieval/data verification such as Text extraction or face recognition need a minimal quality of image for efficient processing and adequate performance. Hence it becomes imperative to implement an Image quality assessment layer before proceeding with further processing. This will ensure smooth applicaton of the vision algorithms, reliable performance and an overall time reduction by ensuring less redundant computations on oor quality images, and prventing multiple requests and passes through the algorithm.&lt;/p&gt;
&lt;p&gt;This additional filter helps by ensuring only optimal quality images are passed on and poor quality images are screened at the client/user stage itself saving the users time and the server unnecessary processing, ensuring higher throughput and efficiency.&lt;/p&gt;
&lt;p&gt;We implemented one such pipeline using an ensemble of models that qualitatively analysed images and produced a quantitative measure for image quality that could then be used as a threshold for decision on whether they are sent for downstream processing or the user is notified to repeat the request with better quality images. This quantitative score ensures flexibility for different tasks and different people tailored to their needs.&lt;/p&gt;
&lt;p&gt;The model detects the blur in an image(
&lt;a href=&#34;&#34;&gt;BlurNet&lt;/a&gt;), brightness of the image(a 
&lt;a href=&#34;&#34;&gt;ResNet-18&lt;/a&gt; model trained for binary classification i.e dark vs bright) and the text readability(based on performance of text detection and OCR algorithms along with other filtering and morphological operations on the image to estimate textual region) and a meta layer performed computation on their individual outputs to provide a final cumulative Image Quality Score.&lt;/p&gt;
&lt;p&gt;The final meta learner was trained taking the outputs of individual models as input with the average image quality scores assigned to each image by annotators being the output score. The annotation was done by assigning each image to atleast five random users and asking them to score the image on the three parameters i.e Blur, Brightness and readability out of 10 solely on their personal discretion. These scores were then fit into a weighting formula to generate a cumulative score. This final score obtained from all the annotators for each image was averaged to output the final ground truth score for the image.&lt;/p&gt;
&lt;p&gt;The clients get both the final score as well as outputs from each individual model along with a short description about the image quality based on the score for analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/misc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/misc/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
