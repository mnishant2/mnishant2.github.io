[{"authors":["admin"],"categories":null,"content":" Hey!! I am currently pursuing my Masters at the School of Computer Science at McGill University. I am also working as a Graduate Research Assistant in collaboration with Center for Intelligent Machines and Department of Diagnostic Radiology.\nI completed my Bachelors from Birla Institute of Technology, India  in Electronics and Communication Engineering. Previously, I have worked as a developer in the AI team of a fintech startup in India Signzy. My interests primarily lie in the domain of Deep Learning, specifically its applications to Computer Vision and Natural Language Processing tasks.\nCurrently, I am working on Deep Learning for Digital Histopathology as part of my Master's thesis. It largely involves using sparse approximation based unsupervised instance segmentation in Whole Slide Images and cross modal feature representation learning from both Histopathology and Radiology data for classification tasks.\nIn the past I have worked on a range of projects as part of coursework, internships and job encompassing diverse tasks in areas of Machine Learning like Face Detection, Optical Character Recognition, Information Retrieval from text, Activity Recognition, Image forensics, Speech Recognition among others.\nI intend to pursue a career in AI research, academic or industrial, in order to push the state of the art in this field and solve the contemporary set of real world problems.   ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1597802042,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/nishant-mishra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nishant-mishra/","section":"authors","summary":"Hey!! I am currently pursuing my Masters at the School of Computer Science at McGill University. I am also working as a Graduate Research Assistant in collaboration with Center for Intelligent Machines and Department of Diagnostic Radiology.","tags":null,"title":"Nishant Mishra","type":"authors"},{"authors":["admin2"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1597802042,"objectID":"ca4549b7186eec7214c27e470e158988","permalink":"/author/nishant-mishra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nishant-mishra/","section":"authors","summary":"","tags":null,"title":"Nishant Mishra","type":"authors"},{"authors":[],"categories":[],"content":"","date":1598016511,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598016511,"objectID":"03b56821ec39c07e58dc185732dd17d0","permalink":"/project/charir/","publishdate":"2020-08-21T09:28:31-04:00","relpermalink":"/project/charir/","section":"project","summary":"","tags":[],"title":"CharIR","type":"project"},{"authors":[],"categories":[],"content":"","date":1598016253,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598016253,"objectID":"0c3ff430fe61cd0572a41a32dc9d94db","permalink":"/project/march-madness/","publishdate":"2020-08-21T09:24:13-04:00","relpermalink":"/project/march-madness/","section":"project","summary":"","tags":[],"title":"March Madness","type":"project"},{"authors":[],"categories":[],"content":"","date":1598016169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598016169,"objectID":"c527e142281e6d14536a4f6e73abbf68","permalink":"/project/speaker_recognition/","publishdate":"2020-08-21T09:22:49-04:00","relpermalink":"/project/speaker_recognition/","section":"project","summary":"","tags":[],"title":"Speaker_recognition","type":"project"},{"authors":[],"categories":[],"content":"","date":1598016049,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598016049,"objectID":"b6b6e6e8a325200162724a5a975cf700","permalink":"/project/cropnet/","publishdate":"2020-08-21T09:20:49-04:00","relpermalink":"/project/cropnet/","section":"project","summary":"","tags":[],"title":"Cropnet","type":"project"},{"authors":[],"categories":[],"content":"","date":1598016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598016000,"objectID":"7b94549be334b87867e5c6d36b8845a5","permalink":"/project/ocr_asr/","publishdate":"2020-08-21T09:20:00-04:00","relpermalink":"/project/ocr_asr/","section":"project","summary":"","tags":[],"title":"OCR_ASR","type":"project"},{"authors":["Nishant Mishra"],"categories":["Computer Vision","layout analysis"],"content":"This project involved an automatic highlighter tool for automatic highlighting and extraction of specific form fields from documents for further processing such as Optical Character Recognition, information retrieval from handwritten documents or even to facilitate semi manual digital population of records from forms using a user interface.\nThe tool utilizes document layout detection, classical Computer vision techniques like template matching and mathematical heuristics to create a generalizable automatic highlighting tool using only one sample of the concerned document.\nThe associated repository here is designed for handling a particular bank form and is a command line highlighting tool that can be appropriated/extended for other documents and interfaces.\n","date":1598015887,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598015887,"objectID":"8a54824059eb2a01e2d09c8a3c54be6f","permalink":"/project/highlighter/","publishdate":"2020-08-21T09:18:07-04:00","relpermalink":"/project/highlighter/","section":"project","summary":"A tool to highlight/extract specific form fields from documents using classical Computer Vision and heuristics","tags":["vision","ocr","text extration"],"title":"Highlighter(Auto field detection)","type":"project"},{"authors":["Nishant Mishra","AB Saravanan"],"categories":["Deep Learning"],"content":"","date":1597725241,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597802042,"objectID":"39398fc5cb4903141e4c8ebf3f61553b","permalink":"/project/dory-ocr/","publishdate":"2020-08-18T00:34:01-04:00","relpermalink":"/project/dory-ocr/","section":"project","summary":"We created an Optical Character Recognition Engine specifically for Indian ID cards using LSTM and CTC loss function.","tags":["vision"],"title":"Dory OCR","type":"project"},{"authors":null,"categories":null,"content":"","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597802042,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"/cv/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/cv/","section":"","summary":"List of Projects,Talks and Publications","tags":null,"title":"CV","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597802042,"objectID":"475f2249a5c02879faf32697c0e89e6e","permalink":"/portfolio/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/portfolio/","section":"","summary":"List of Projects,Talks and Publications","tags":null,"title":"Portfolio","type":"widget_page"},{"authors":["Shubham Chopra","Nishant Mishra"],"categories":["Reinforcement Learning","course project"],"content":"This project was done as part of my final project submission for COMP767: Reinforcement Learning COMP767: Reinforcement Learning course at McGill University\nIn the recent years, significant work has been done in the field of Deep Reinforcement Learning, to solve challenging problems in many diverse domains. One such example, are Policy gradient algorithms, which are ubiquitous in state-of-the-art continuous control tasks. Policy gradient methods can be generally divided into two groups: off-policy gradient methods, such as Deep Deterministic Policy Gradients (DDPG) Deep Deterministic Policy Gradients (DDPG) , Twin Delayed Deep Deterministic (TD3) Twin Delayed Deep Deterministic (TD3) , Soft Actor Critic (SAC) Soft Actor Critic (SAC) and on-policy methods, such as Trust Region Policy Optimization (TRPO) Trust Region Policy Optimization (TRPO) .\nHowever, despite these successes on paper, reproducing deep RL results is rarely straightforward. There are many sources of possible instability and variance including extrinsic factors (such as hyper-parameters, noise-functions used) or intrinsic factors (such as random seeds, environment properties).\nIn this project, we perform two different analysis on these policy gradient methods: (i) Reproduction and Comparison: We implement a variant of DDPG, based on the original paper. We then attempt to reproduce the results of DDPG (our implementation) and TD3 and compare them with the well-established methods of REINFORCE and A2C. (ii) Hyper-Parameter Tuning: We also, study the effect of various Hyper-Parameters(namely Network Size, Batch Sizes) on the performance of these methods.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"9a067d49836df8e7d7f1a8a570b04d0e","permalink":"/project/policy_gradient/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/project/policy_gradient/","section":"project","summary":"Reproducibility and Analysis of Deep Policy Gradient methods for Reinforcement Learning Tasks","tags":["RL","Policy Gradients"],"title":"Policy Gradient","type":"project"},{"authors":["Paniz Bertsch","Albert Orozco Camacho","Nishant Mishra"],"categories":["Graph Representation Learning"],"content":"This project was undertaken as part of the final project for COMP 766: Graph Representation Learning COMP 766: Graph Representation Learning course at McGill University.\nFor many computer science sub-fields, knowledge graphs (KG) remain a constant abstraction whose usefulness relies in their representation power. However, dynamic environments, such as the temporal streams of social media information, brings a greater necessity of incorporating additional structures to KGâ€™s.\nIn this project, we applied currently available solutions to address incremental knowledge graph embedding to several applications to test their efficiency. We also proposed an embedding model agnostic framework to make these models incremental. Firstly, we proposed a window-based incremental learning approach that discards least happening facts and performs link prediction on updated triples. Next, we presented experiments on a GCN model-agnostic meta-learning based approach.\nTo create edge embedding vectors, we experimented with two methods:\n Concatenating head and tailâ€™s 128-dimensional Node2Vec embedding vectors to create 256-dimensional edge embedding Subtracting head embedding from tail embedding vector to create 128-dimensional edge embedding vector Our best model is the Window-based KG Incremental Learning, where edge representations, are calculated from subtraction of embedding vectors of head and tail nodes  For the experiment, link prediction adjusted to a binary classification, with 0 and 1 representing link is present or absent respectively, was used, with Random-Forest model for training and prediction. Also, dataset is divided to training set and nine test sets as incremental updates, to generate 9 snapshots of graph with each snapshot, adding new nodes and updating edges compare to previous graph snapshot.\nThe second method we experimented with followed a model-agnostic meta-learning based approach with Graph Convolutional Networks(GCN). The idea here is to learn a GCN to predict the embeddings of new nodes given the old embeddings of its neighboring entities in the old graph and similarly obtain an updated representation of old entities based on the recently learned embedding of new entities. These two predictions are jointly iterated. This can be viewed as learning to learn problem (meta-learning).    tsne visualization of top 40 entity embeddings cluster\n                               ","date":1586870617,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586870617,"objectID":"c85051f0976a751798eef9680b451936","permalink":"/project/online_learning/","publishdate":"2020-04-14T09:23:37-04:00","relpermalink":"/project/online_learning/","section":"project","summary":"In this project, we apply currently available solutions to address incremental knowledge graph embedding to several applications to test their efficiency.","tags":["graph","knowledge bases","Other","gcn","fb20k"],"title":"Online Learning of temporal Knowledge Graphs","type":"project"},{"authors":["Priyesh Vijayan","Ashita Diwan","Nishant Mishra"],"categories":["graph representation learning","nlp","course project"],"content":"This project was directed towards the final course project requirement for COMP 550: Natural Language Processing COMP 550: Natural Language Processing course at McGill University.\nKnowledge graphs (KGs) succinctly represent real-world facts as multi-relational graphs. A plethora of work exists in embedding the information in KG to a continuous vector space in order to obtain new facts and facilitate multiple down-stream NLP tasks.\nDespite the popularity of the KG embedding problem, to the best of our knowledge, we find that no existing work handles dynamic/evolving knowledge graphs that incorporates facts about new entities.\nIn this project, we propose this problem as an incremental learning problem and propose solutions to obtain representations for new entities and also update the representations of old entities that share facts with these newer entities. The primary motive of this setup is to avoid relearning the knowledge graph embedding altogether with the occurrence of every new set of facts (triplets).\nWe build our solutions with TransE(Bordes et al.) TransE(Bordes et al.) as our base KG embedding model and evaluate the learned embeddings on facts associated with these new entities.\nTo this aim, we formulated two solutions; the first approach followed a finetuning based transfer-learning solution, and the second followed a model-agnostic meta-learning based approach with Graph Convolutional Networks (GCN). While our model-specific finetuning approach fared well, the proposed model independent approach failed to learn representations for a new entity.\nWe used OpenKEâ€™s OpenKEâ€™s implementation for setting our model. For our task, we made changes to the TransE model, so that it can learn the representations of the new entities. We employed the FB20K FB20K dataset ( Xie et al., 2016 Xie et al., 2016 ) for our task. In addition to containing all the entities and relations from the FB15K dataset, this dataset also contains new entities which was required for our setup. We evaluate the models for link prediction, which aims to predict the missing h or t for a relation fact (h, r, t).\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"9292e43774e11c9f7cbc975f7c85919c","permalink":"/project/transe/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/project/transe/","section":"project","summary":"In this project, we propose an incremental learning problem for Knowledge Graphs to obtain representations for new entities and also update the representations of old entities that share facts with these newer entities.","tags":["nlp","graph represeantation learning","GCN","TransE"],"title":"Incremental Knowledge Graphs","type":"project"},{"authors":[],"categories":["deep learning","nlp"],"content":"The project at Signzy involved training a generalizable model for information retrieval from OCR output of Indian ID cards. We used both character level embeddings and word level embeddings( ELMO ELMO ) in a stacked manner for language modelling before passing the concatenated embeddings to a bidirectional Long Short Term Memory neural network with Conditional Random Field modelling on LSTM output ( Huang et al. Huang et al. ) for final classification.\nThe model was trained on a large corpus of text OCR outputs obtained from our own proprietary ID cards dataset for extracting non-trivial information such as Names, dates, numbers, addreses from any card. The training was done in a way to ensure the embeddings were also fine tuned. The FlairNLP library FlairNLP library was used to create the preprocessing, text embedding, training and postprocessing pipeline and training was performed using pytorch framework. Multiple combinations of embeddings including FlairEmbeddings( Contextualized string embeddings for sequence labelling Contextualized string embeddings for sequence labelling ), BERT, CharacterEmbeddings, ELMO, XLNet were benchmarked before settling on the final pair based on accuracy, compute and efficiency considerations.\nNot only did the model perform admirably well on unseen text from ID types part of training data irrespective of variations in OCR output and image layout, but it generalised well for out of sample ID types too when finetuned with just 1-5 samples of these cards.\nThe idea behind this was to build a generic, flexible information retrieval engine thats pretrained to extract important information from OCR output of all ID cards without specifically being trained on them or having seen them, without any rule based processing, that can be easily finetuned on a very small number of samples of any new card type for optimum performance. This was made into a rest API as a plug and play product for clients to finetune the model on their samples and then use it out of the box to extract information from IDs. The performance was measured using precision and recall figures.\n","date":1558444841,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558444841,"objectID":"f652a66d3c7e33390d39fe1c6bdd2a7c","permalink":"/project/gem/","publishdate":"2019-05-21T09:20:41-04:00","relpermalink":"/project/gem/","section":"project","summary":"Trained a biLSTM model using both word and character level embeddings for information retrieval from text OCR outputs of ID cards","tags":["nlp"],"title":"Generic Extraction Module (G.E.M)","type":"project"},{"authors":["Nishant Mishra","AB Saravanan","Christen Miller"],"categories":["vision","ensemble learning"],"content":"Many of the vision based applications or APIs meant for information retrieval/data verification such as Text extraction or face recognition need a minimal quality of image for efficient processing and adequate performance. Hence it becomes imperative to implement an Image quality assessment layer before proceeding with further processing. This will ensure smooth applicaton of the vision algorithms, reliable performance and an overall time reduction by ensuring less redundant computations on oor quality images, and prventing multiple requests and passes through the algorithm.\nThis additional filter helps by ensuring only optimal quality images are passed on and poor quality images are screened at the client/user stage itself saving the users time and the server unnecessary processing, ensuring higher throughput and efficiency.\nWe implemented one such pipeline using an ensemble of models that qualitatively analysed images and produced a quantitative measure for image quality that could then be used as a threshold for decision on whether they are sent for downstream processing or the user is notified to repeat the request with better quality images. This quantitative score ensures flexibility for different tasks and different people tailored to their needs.\nThe model detects the blur in an image( BlurNet BlurNet ), brightness of the image(a ResNet-18 ResNet-18 model trained for binary classification i.e dark vs bright) and the text readability(based on performance of text detection and OCR algorithms along with other filtering and morphological operations on the image to estimate textual region) and a meta layer performed computation on their individual outputs to provide a final cumulative Image Quality Score.\nThe final meta learner was trained taking the outputs of individual models as input with the average image quality scores assigned to each image by annotators being the output score. The annotation was done by assigning each image to atleast five random users and asking them to score the image on the three parameters i.e Blur, Brightness and readability out of 10 solely on their personal discretion. These scores were then fit into a weighting formula to generate a cumulative score. This final score obtained from all the annotators for each image was averaged to output the final ground truth score for the image.\nThe clients get both the final score as well as outputs from each individual model along with a short description about the image quality based on the score for analysis.\n","date":1548028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548028800,"objectID":"ca1dccfb31083acf903d05d61fa19fa3","permalink":"/project/iqa/","publishdate":"2019-01-21T00:00:00Z","relpermalink":"/project/iqa/","section":"project","summary":"An ensemble model to quantify image quality to filter poor quality images at the client end to prevent redundant processing","tags":["vision","blur","brightness","text readability","resnet18","ocr"],"title":"IQA","type":"project"},{"authors":["Nishant Mishra","Rohit Mohan"],"categories":["vision","deep learning"],"content":"This work was selected for and presented at the final round of Smart India Hackathon 2017 by government of India. The project involved implementing a proof of concept system to detect anomalous activities from camera feed. For this purpose we used the Database for recognition of human actions Database for recognition of human actions from the Computer Science department at KTH Royal institute of technology.\nThe database consists of seven types of human actions (walking, jogging, running, boxing, hand waving, sliding and hand clapping) performed several times by 25 subjects in four different scenarios: outdoors:s1, outdoors with scale variation:s2, outdoors with different clothes:s3 and indoors:s4. All sequences were taken over homogeneous backgrounds with a static camera with 25fps frame rate. The sequences were downsampled to the spatial resolution of 160x120 pixels and have a length of four seconds in average.\nThe SOP of the project was preprocessing and feature extraction from the sequences to be passed on for training. All the frames were smoothed with a gaussian filter. This was followed by contour detection. A novel approach of pooling extracted contours(green boxes in video) after Mixture of Gaussian based Background subtraction Mixture of Gaussian based Background subtraction to get an aggregate binary boundary image of the foreground(contour of the subjects)(blue bounding box in video) as features was implemented.\nIn order to account for the temporal aspect, these final contour images were aggregated in batches of five consecutive frames to be passed on to the Neural Network for training. Additional quantities such as centroid, median topmost, bottommost coordinates of the contours, and squared differences of consecutive left and right coordinates were also claculated for the batch of five frames and passed on to represent the speed and posture. All of these features were concatenated and Principal Component Analysis Principal Component Analysis was applied to them for reducing the dimensionality with n_components=100 that captured most of the variance in the feature space while minimizing the dimension and hence computation and storage requirements. The features were than stored using cPickle.\nBoth Fully connected neural network and CNN were used for training with comparable performance, with an accuracy of ~96% for classification of activities as anomaalous or normal. From the above mentioned actions, boxing and sliding were grouped as anomalous activities and the rest 5 as non anomalous.\n","date":1508025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508025600,"objectID":"2c27cf46762e06a5d720c1bdf91fbb40","permalink":"/project/activity_recognition/","publishdate":"2017-10-15T00:00:00Z","relpermalink":"/project/activity_recognition/","section":"project","summary":"Using traditional computer vision with deep learning algorithms for Anomalous activity detection from CCTV camera feed","tags":["vision","deep learning","hackathon","SIH","KTH","PCA","opencv"],"title":"Activity Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597802042,"objectID":"3ef1c3ed755398dc4fffccfec12a9a68","permalink":"/misc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/misc/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]