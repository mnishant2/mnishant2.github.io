<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shubham Chopra | Nishant Mishra</title>
    <link>/author/shubham-chopra/</link>
      <atom:link href="/author/shubham-chopra/index.xml" rel="self" type="application/rss+xml" />
    <description>Shubham Chopra</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020</copyright><lastBuildDate>Fri, 01 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Shubham Chopra</title>
      <link>/author/shubham-chopra/</link>
    </image>
    
    <item>
      <title>Policy Gradient</title>
      <link>/project/policy_gradient/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>/project/policy_gradient/</guid>
      <description>&lt;p&gt;This project was done as part of my final project submission for 
&lt;a href=&#34;https://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COMP767: Reinforcement Learning&lt;/a&gt;
&lt;a href=&#34;https://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html&#34; target=&#34;_blank&#34;&gt;COMP767: Reinforcement Learning&lt;/a&gt;
 course at McGill University&lt;/p&gt;
&lt;p&gt;In the recent years, significant work has been done in the field of Deep Reinforcement
Learning, to solve challenging problems in many diverse domains. One such example,
are Policy gradient algorithms, which are ubiquitous in state-of-the-art continuous control
tasks. Policy gradient methods can be generally divided into two groups: off-policy
gradient methods, such as 
&lt;a href=&#34;https://arxiv.org/abs/1509.02971&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Deterministic Policy Gradients (DDPG)&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1509.02971&#34; target=&#34;_blank&#34;&gt;Deep Deterministic Policy Gradients (DDPG)&lt;/a&gt;
, 
&lt;a href=&#34;https://arxiv.org/pdf/1802.09477.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twin Delayed
Deep Deterministic (TD3)&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1802.09477.pdf&#34; target=&#34;_blank&#34;&gt;Twin Delayed
Deep Deterministic (TD3)&lt;/a&gt;
, 
&lt;a href=&#34;https://arxiv.org/abs/1801.01290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Soft Actor Critic (SAC)&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1801.01290&#34; target=&#34;_blank&#34;&gt;Soft Actor Critic (SAC)&lt;/a&gt;
 and on-policy methods, such as

&lt;a href=&#34;https://arxiv.org/abs/1502.05477&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trust Region Policy Optimization (TRPO)&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1502.05477&#34; target=&#34;_blank&#34;&gt;Trust Region Policy Optimization (TRPO)&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;However, despite these successes on paper, reproducing deep RL results is rarely straightforward. There are many sources of possible instability and variance including extrinsic
factors (such as hyper-parameters, noise-functions used) or intrinsic factors (such as random
seeds, environment properties).&lt;/p&gt;
&lt;p&gt;In this project, we perform two different analysis on these policy gradient methods:
(i) Reproduction and Comparison: We implement a variant of DDPG, based on the original
paper. We then attempt to reproduce the results of DDPG (our implementation) and
TD3 and compare them with the well-established methods of REINFORCE and A2C.
(ii) Hyper-Parameter Tuning: We also, study the effect of various Hyper-Parameters(namely
Network Size, Batch Sizes) on the performance of these methods.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
