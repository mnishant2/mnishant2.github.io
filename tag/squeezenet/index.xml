<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>squeezenet | Nishant Mishra</title>
    <link>/tag/squeezenet/</link>
      <atom:link href="/tag/squeezenet/index.xml" rel="self" type="application/rss+xml" />
    <description>squeezenet</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© NM 2020</copyright><lastBuildDate>Thu, 12 Apr 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/dab.jpg</url>
      <title>squeezenet</title>
      <link>/tag/squeezenet/</link>
    </image>
    
    <item>
      <title>Cropnet</title>
      <link>/project/cropnet/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/project/cropnet/</guid>
      <description>&lt;p&gt;As the name suggests, this project involved training a model to crop out documents from a background. Essentially this can be classified as a segmentation task that would need massive annotation of data with a mask on the foreground object which is to be used for supervised segmentation training.&lt;/p&gt;
&lt;p&gt;We decided to cast this into a regression based problem where we annotated only the four corner points of the foreground object as our training labels and then used them to train a regression model with 8 continuous valued outputs({x,y} coordinates of all four corners). Once we had these points we implemented a perspective transform to warp the object into a rectangular space for the final cropped output.&lt;/p&gt;
&lt;p&gt;For training we used custom aggregated and crowdsourced dataset of ID cards and other documents in various background settings. We implemented our own 
&lt;a href=&#34;https://github.com/mnishant2/NMAnnotation-tool&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annotation tool&lt;/a&gt;

 for the above mentioned ground truth annotation. In order to ensure variance, we used both natural camera taken images as well as synthetically generated data by superimposing the already available cropped samples on random backgrounds at different positions, scales and orientation.&lt;/p&gt;
&lt;p&gt;Not only this, we also implemented massive data augmentation in order to further multiply our training data that worked simultaneously on the image and the annotated keypoints. Some of the augmentation techniques used were blurring, rotation, scaling,grayscale, color adjustments, dropout, adding noise etc. We used the 
&lt;a href=&#34;https://imgaug.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;imgaug library&lt;/a&gt;

 for the whole augmentation pipeline.&lt;/p&gt;
&lt;p&gt;Annotation, synthetic data generation, and augmentation were all done in such a way as to ensure the sequence of the four corner points with respect to the object remained same in order to ensure spatial and rotational invariance during training and prediction. The upper left point of the foreground object was always the first label followed by others in a clockwise manner.&lt;/p&gt;
&lt;p&gt;Once we had sufficient annotated and augmented data, we trained the regression models. We experimented and benchmarked a number of different algorithms and learning paradigms. We benchmarked 
&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResNet&lt;/a&gt;

, 
&lt;a href=&#34;https://arxiv.org/abs/1602.07360&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Squeezenet&lt;/a&gt;

, 
&lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VGGNet&lt;/a&gt;

, 
&lt;a href=&#34;https://arxiv.org/abs/1707.01083&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shufflenet&lt;/a&gt;

 in both transfer learning and from scratch settings for a large range of hyperparameter values and benchmarked their performances.&lt;/p&gt;
&lt;p&gt;The outputs, as mentioned above were eight continuous labels, hence the final layer was always a linear activation layer. The loss functions used were variations of Mean Squared Error values. This whole concept was applied and tested on a number of applications such as cropping ID cards from background for further processing in an 
&lt;a href=&#34;../Dory-OCR/&#34;&gt;Optical Character Recognition system&lt;/a&gt;

, cropping 
&lt;a href=&#34;&#34;&gt;Cheque MICR&lt;/a&gt;

 stub for MICR extraction for digital processing, Passport MRZ code extraction, scanned document layout detection etc and they all fit in perfectly in the overall pipeline and gave robust performances.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
