<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ocr | Nishant Mishra</title>
    <link>/tag/ocr/</link>
      <atom:link href="/tag/ocr/index.xml" rel="self" type="application/rss+xml" />
    <description>ocr</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020</copyright><lastBuildDate>Fri, 21 Aug 2020 09:18:07 -0400</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>ocr</title>
      <link>/tag/ocr/</link>
    </image>
    
    <item>
      <title>Highlighter(Auto field detection)</title>
      <link>/project/highlighter/</link>
      <pubDate>Fri, 21 Aug 2020 09:18:07 -0400</pubDate>
      <guid>/project/highlighter/</guid>
      <description>&lt;p&gt;This project involved an automatic highlighter tool for automatic highlighting and extraction of specific form fields from documents for further processing such as Optical Character Recognition, information retrieval from handwritten documents or even to facilitate semi manual digital population of records from forms using a user interface.&lt;/p&gt;
&lt;p&gt;The tool utilizes document layout detection, classical Computer vision techniques like template matching and mathematical heuristics to create a generalizable automatic highlighting tool using only one sample of the concerned document.&lt;/p&gt;
&lt;p&gt;The associated repository here is designed for handling a particular bank form and is a command line highlighting tool that can be appropriated/extended for other documents and interfaces.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IQA</title>
      <link>/project/iqa/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/project/iqa/</guid>
      <description>&lt;p&gt;Many of the vision based applications or APIs meant for information retrieval/data verification such as Text extraction or face recognition need a minimal quality of image for efficient processing and adequate performance. Hence it becomes imperative to implement an Image quality assessment layer before proceeding with further processing. This will ensure smooth applicaton of the vision algorithms, reliable performance and an overall time reduction by ensuring less redundant computations on oor quality images, and prventing multiple requests and passes through the algorithm.&lt;/p&gt;
&lt;p&gt;This additional filter helps by ensuring only optimal quality images are passed on and poor quality images are screened at the client/user stage itself saving the users time and the server unnecessary processing, ensuring higher throughput and efficiency.&lt;/p&gt;
&lt;p&gt;We implemented one such pipeline using an ensemble of models that qualitatively analysed images and produced a quantitative measure for image quality that could then be used as a threshold for decision on whether they are sent for downstream processing or the user is notified to repeat the request with better quality images. This quantitative score ensures flexibility for different tasks and different people tailored to their needs.&lt;/p&gt;
&lt;p&gt;The model detects the blur in an image(
&lt;a href=&#34;&#34;&gt;BlurNet&lt;/a&gt;

), brightness of the image(a 
&lt;a href=&#34;&#34;&gt;ResNet-18&lt;/a&gt;

 model trained for binary classification i.e dark vs bright) and the text readability(based on performance of text detection and OCR algorithms along with other filtering and morphological operations on the image to estimate textual region) and a meta layer performed computation on their individual outputs to provide a final cumulative Image Quality Score.&lt;/p&gt;
&lt;p&gt;The final meta learner was trained taking the outputs of individual models as input with the average image quality scores assigned to each image by annotators being the output score. The annotation was done by assigning each image to atleast five random users and asking them to score the image on the three parameters i.e Blur, Brightness and readability out of 10 solely on their personal discretion. These scores were then fit into a weighting formula to generate a cumulative score. This final score obtained from all the annotators for each image was averaged to output the final ground truth score for the image.&lt;/p&gt;
&lt;p&gt;The clients get both the final score as well as outputs from each individual model along with a short description about the image quality based on the score for analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OCR to enrich ASR</title>
      <link>/project/ocr_asr/</link>
      <pubDate>Wed, 31 May 2017 09:20:00 -0400</pubDate>
      <guid>/project/ocr_asr/</guid>
      <description>&lt;p&gt;Automatic Speech Recognition systems, especially those leveraging probabilistic modeling such as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Hidden_Markov_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hidden Markov Model&lt;/a&gt;

 based ASR systems rely a lot on the associated data/lexicon for optimum performance.
In this project done as part of my undergrad summer research Internship at 
&lt;a href=&#34;https://www.irit.fr/en/home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Recherche en Informatique de Toulouse (IRIT)&lt;/a&gt;

, Universite Paul Sabatier, we intended to analyse the possible boost in ASR performance by incorporating output of Optical Character Recognition applied on associated visual components of the speech.&lt;/p&gt;
&lt;p&gt;We set out to study the impact of populating the lexicon of speech processing system with OCR outputs obtained from their videos. To this end, we used the open source, readily available MOOC data for the experimentation. Performing Automatic Speech recognition on these lectures for transcription and indexing is a bit difficult because different videos have a specific set of words depending on the domain of the video called jargon,which are not present in general lexicons we use to train speech recognition models. But most of these videos also have text as part of slides or handwritten scribbles on screen which if used to populate the lexicon in realtime will benefit the speech recognition system.&lt;/p&gt;
&lt;p&gt;We set out by creating a corpus of such videos along with their transcripts with timestamps and the slides used in pdf or other file formats. We used 
&lt;a href=&#34;https://tika.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;apache Tika&lt;/a&gt;

 to extract text from these slides as part of ground truth. We also implemented a semi automatic GUI to annotate the slide transitions with respective timestamps in the video for accurate temporal alignement with ground truth for benchmarking OCR performance.&lt;/p&gt;
&lt;p&gt;For Video OCR we used the 
&lt;a href=&#34;https://www.semanticscholar.org/paper/From-Text-Detection-in-Videos-to-Person-Poignant-Besacier/f192f2461702c84c5ffb4253dc316b626f26b6df&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LOOV(Poignant et al.)&lt;/a&gt;

 tool that uses classical Computational techniques such as Sobel filtering, 
&lt;a href=&#34;https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_niblack_sauvola.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sauvola Algorithm&lt;/a&gt;

 followed by text tracking over consecutive frame to ensure text persistence for text detection and then 
&lt;a href=&#34;https://github.com/tesseract-ocr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tesseract OCR engine&lt;/a&gt;

 for text detection. The text detections are averaged over shifted regions and Viterbi Algorithm applied for modelling the best OCR output using 
&lt;a href=&#34;http://www.speech.sri.com/projects/srilm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SRILM library&lt;/a&gt;

. We reimplemented parts of LOOV in python by taking developers version of PyLOOV which had functional issues and optimised it for our own use case.&lt;/p&gt;
&lt;p&gt;We benchmarked the performance of our video OCR using ground truth annotations obtained from the slides using Recall and precision as metrics. Now we identified some domain specific words that were present in the OCR output but not in the transcript to get a general ballpark of possible improvement. We found out that there were words in range of 2 to 20%(avg ~10%) of the total words in the OCR which were absent in the transcripts on a per slide basis. The HMM based speech Recognition model was trained with the old and updated lexicons using 
&lt;a href=&#34;https://kaldi-asr.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaldi toolkit&lt;/a&gt;

 and expectedly we observed a significant improvement of an average of 5% in performance of the ASR for our dataset which were heavily domain oriented course lectures from Online course websites such as Coursera, edX.&lt;/p&gt;
&lt;p&gt;Such a tool when integrated in ASR systems to update lexicon real time would help tremendously improve the ASR output.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
