<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>laplacian | Nishant Mishra</title>
    <link>https://mnishant2.github.io/tag/laplacian/</link>
      <atom:link href="https://mnishant2.github.io/tag/laplacian/index.xml" rel="self" type="application/rss+xml" />
    <description>laplacian</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© NM 2020</copyright><lastBuildDate>Mon, 11 Nov 2019 14:11:11 -0400</lastBuildDate>
    <image>
      <url>https://mnishant2.github.io/media/dab.jpg</url>
      <title>laplacian</title>
      <link>https://mnishant2.github.io/tag/laplacian/</link>
    </image>
    
    <item>
      <title>SIFT</title>
      <link>https://mnishant2.github.io/project/sift/</link>
      <pubDate>Mon, 11 Nov 2019 14:11:11 -0400</pubDate>
      <guid>https://mnishant2.github.io/project/sift/</guid>
      <description>&lt;p&gt;In this project, which was essentially an assignment in 
&lt;a href=&#34;https://www.mcgill.ca/study/2018-2019/courses/comp-558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COMP558:Fundamentals of Computer Vision&lt;/a&gt;

 course, I implemented the 
&lt;a href=&#34;http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scale Invariant Feature Transform(SIFT)&lt;/a&gt;

 algorithm from scratch. SIFT is a traditional computer vision feature extraction technique. SIFT features are scale, space and rotationally invariant.&lt;/p&gt;
&lt;p&gt;SIFT is a highly involved algorithm and thus implementing it from scratch is an arduous tasks. At an abstract level the SIFT algorithm can be described in five steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Find Scale Space Extrema&lt;/strong&gt;: We construct the 
&lt;a href=&#34;https://www.sciencedirect.com/topics/engineering/laplacian-pyramid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laplacian(Difference of Gaussian) pyramid&lt;/a&gt;

 for the given image and using this pyramid, we
found local extremas in each level of the laplacian pyramid by taking a local area and
comparing the intensities in that local region for the same scale as well as the
adjacent(next and previous) levels in the pyramid. Two local
neighbourhood sizes(3&lt;em&gt;3,5&lt;/em&gt;5) were tried.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Keypoint Localization&lt;/strong&gt;: A large number of keypoints are generated by the first step which might not be useful. Corner cases and low contrast keypoints are discarded. Also a threshold was specified in order to select only strong extremas. A 
&lt;a href=&#34;https://mathworld.wolfram.com/TaylorSeries.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;taylor series expansion&lt;/a&gt;

 of scale space is done to get a more accurate value of extrema and those falling below the threshold were discarded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient Calculation&lt;/strong&gt;: For each keypoint detected, a square neigborhood(17x17 in our case) was taken around them at their respective scales. Intensity gradients and orientation were calculated for the given neighborhood. A 
&lt;a href=&#34;https://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gaussian mask&lt;/a&gt;

 of the same size as our neighborhood was used as a weighting mask over gradient magnitude matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SIFT Feature Descriptors&lt;/strong&gt;: SIFT feature descriptors are created by taking 
&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;histograms of gradients orientations&lt;/a&gt;

 for each keypoint neighborhoods. Orientations are divided into bins of various ranges(36 bins of 10 deg in our case), and for each gradient falling in a bin the gradient magnitude value is added to that particular bin. Once we have the histogram we find the orientation with the highest weighted value. Its the principle orientation and the desriptors(orientation vectors) are shifted counterclockwise such that principle orientation becomes the first bin. This lends SIFT features their rotational invariance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once we had the SIFT desriptors, I transformed the image and calculated SIFT vectors for the original and transformed images and matched them using bruteforce algorithm i.e 
&lt;a href=&#34;https://www.sciencedirect.com/topics/engineering/bhattacharyya-distance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bhattacharyya Distance&lt;/a&gt;

 and visualised(as in figure above) the matches above a certain threshold to test the robustness of the SIFT algorithm.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
